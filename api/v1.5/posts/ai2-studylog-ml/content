{"code":0,"data":{"title":"人工智能笔记 - 机器学习","content":"# 人工智能笔记 - 机器学习\n\n# 人工智能 - 机器学习[ #](#人工智能---机器学习)\n\n人工智能的基础是机器学习，顾名思义就是让机器能够处理数据，并在处理的数据中进行学习，从而能够自主地做出决策。简单来说，机器学习是让机器能够在不需要特别编程的情况下进行学习和改进。\n\n而机器学习，大体上分为三种，分别为：监督式学习，无监督式学习，和深度学习。\n\n## 监督式学习[ #](#监督式学习)\n\n监督式学习是一种机器学习方法，它通过使用带有标签的训练数据（也就是已知的输入和输出）来学习一种模型或函数。然后，这种模型或函数可以用于预测新的、未知的数据。在训练过程中，如果模型的预测结果与实际结果出现差异，就会进行调整，以减小这种差异。这种学习方式就好比有一位“监督者”指导机器如何进行学习。\n\n监督式学习自然离不开训练算法，它的算法特征是需要输入输出，并且有反馈的输出结果（并且监督式学习的数据都是有标签的数据）。\n\n常见的监督式学习算法有线性回归，逻辑回归，决策树，支持向量机（SVM），以及K-NN算法（最相邻算法）。\n\n### 线性回归[ #](#线性回归)\n\n线性回归是一种常见的预测分析方法，适用于两个或更多变量之间存在线性关系的情况。在机器学习中，线性回归的目标是找到一条最佳拟合线，使得所有数据点到这条线的垂直距离（也就是误差）之和最小。这个过程也被称为最小二乘法。\n\n线性回归模型的一般形式是 Y = aX + b，其中，Y 是因变量（我们想要预测的目标），X 是自变量（我们用来进行预测的输入），a 和 b 是模型参数，它们决定了线的斜率和截距。\n\n线性回归提供了一种量化自变量和因变量之间关系的方式，是很多更复杂模型和技术的基础。它在多个领域都有广泛的应用，包括经济、生物、环境和社会科学等。\n\n下面以一个例子示范：\n\npythonCopy\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\n\ndef f(x):\n    np.random.seed(42)\n    y = 0.1 * x + 1.25 + 0.2 * np.random.randn(x.shape[0])\n    return y\n\nx = np.arange(0, 20, 0.5)\ny = f(x)\n\nplt.scatter(x, y, label='Actual Data', color='blue')\nplt.title('Data')\nplt.legend()\n```\n\n上面的代码，生成了一些散列点，表示一系列的算计事件，如图：\n\n![Screenshot 2024-07-01 at 00.48.33.png](/post/ai2-studylog-ml/08429d71f88f.png)\n\n这样样本数据就准备好了。\n\n下面就是进行模型训练了，使用机器学习库中的线性回归算法，并进行模型训练（打印了斜率系数和截距）：\n\npythonCopy\n\n```python\nfrom sklearn.linear_model import LinearRegression\n\nregression_lineal = LinearRegression()\nregression_lineal.fit(x.reshape(-1, 1), y)\n\nprint('w = ' + str(regression_lineal.coef_) + ', b = ' + str(regression_lineal.intercept_))\n```\n\npythonCopy\n\n```python\nw = [0.09183522], b = 1.2858792525736682\n```\n\n之后，再进行预测（这里输入值是5，和图中的点的位置比较接近）\n\npythonCopy\n\n```python\nnew_x = np.array([5])\nprediction = regression_lineal.predict(new_x.reshape(-1, 1))\nprint(prediction)\n```\n\npythonCopy\n\n```python\n[1.74505534]\n```\n\n除此之外，还可以计算出来模型的评估参数，这里计算了标准差和R2的值：\n\n![Screenshot 2024-07-01 at 01.00.18.png](/post/ai2-studylog-ml/975edaa12afe.png)\n\n可见，这里的得分接近0.9，说明模型适配比较不错，最后再看一下这条拟合线：\n\n![Screenshot 2024-07-01 at 01.04.02.png](/post/ai2-studylog-ml/16783c57471c.png)\n\n### 逻辑回归[ #](#逻辑回归)\n\n逻辑回归是一种预测分析的统计技术，用于解决二分类问题。它通过使用逻辑函数将线性回归的输出映射到0和1之间，这使得其成为一种处理分类问题的好工具。逻辑回归的主要目标是找到一个将输入变量和输出之间关系最好的逻辑函数，这个函数的输出可以被解释为某一事件发生的概率。\n\n逻辑回归模型的一般形式是 p = 1 / (1 + e^(-z))，其中，p 是预测的概率，z = aX + b，a 和 b 是模型参数，X 是输入变量。在训练过程中，我们需要找到这样的 a 和 b，使得预测的概率和实际的类别标签之间的差异最小。这个过程通常通过最大似然估计或梯度下降算法来完成。\n\n逻辑回归对于理解影响结果的因素、预测结果、以及创建简单的分类模型都非常有用。它在金融、医疗、社会科学等多个领域都有广泛的应用。\n\n### 决策树[ #](#决策树)\n\n决策树是一种用于分类和回归的树状结构的机器学习模型。它通过一系列的分裂规则将数据集划分成不同的子集，并最终形成一个树形结构，每个节点代表一个特征，每个分支代表该特征的一个可能值，每个叶子节点代表一个分类或回归结果。\n\n决策树的主要优点包括：\n\n1. **易于理解和解释**：决策树模型的结构直观，便于解释和理解，适合可视化展示。\n2. **少量的数据预处理**：决策树不需要对数据进行过多的预处理，如归一化、填补缺失值等。\n3. **处理多种数据类型**：决策树可以处理数值型和分类型数据，具有较强的灵活性。\n\n决策树的构建过程通常包括以下几个步骤：\n\n1. **选择最优特征**：根据某种度量标准（如信息增益、基尼指数等）选择最优的特征进行分裂。\n2. **创建分裂点**：根据选定的特征创建分裂点，将数据划分为若干子集。\n3. **递归分裂**：对每一个子集递归地重复步骤1和2，直到满足停止条件（如达到最大树深度或叶子节点中的样本数小于某个阈值）。\n\n决策树的缺点包括：\n\n1. **容易过拟合**：决策树容易对训练数据过拟合，特别是当树过深时，可以通过剪枝或设置最大深度等方法来缓解。\n2. **对噪声敏感**：决策树对噪声数据较为敏感，噪声数据可能导致树的不稳定。\n\n决策树在很多实际应用中表现良好，如信用评分、医疗诊断、市场分析等。它还可以与其他模型结合，如在随机森林和梯度提升树等集成学习方法中。\n\n下面是一个简单的决策树分类示例：\n\n![Screenshot 2024-07-04 at 01.29.11.png](/post/ai2-studylog-ml/ed5c515526f0.png)\n\n![Screenshot 2024-07-04 at 01.30.18.png](/post/ai2-studylog-ml/058f8c213f29.png)\n\n![Screenshot 2024-07-04 at 01.30.48.png](/post/ai2-studylog-ml/067ad6d7e168.png)\n\n### K-NN算法（最相邻算法）[ #](#k-nn算法最相邻算法)\n\nK-NN算法，全称为K最近邻算法（K-Nearest Neighbors），是一种简单且直观的分类和回归方法。K-NN算法的基本思想是：给定一个待分类的样本，找到训练集中距离该样本最近的K个样本，然后根据这K个样本的类别，通过投票或平均的方法决定待分类样本的类别。\n\nK-NN算法的步骤如下：\n\n1. **选择参数K**：选择一个正整数K，表示要考虑的最近邻居的数量。\n2. **计算距离**：对待分类样本，与训练集中每一个样本计算距离。常用的距离度量方法有欧氏距离、曼哈顿距离等。\n3. **选择最近的K个邻居**：根据计算出的距离，从训练集中选择距离最近的K个样本。\n4. **投票或平均**：对K个最近邻的样本进行投票，选择出现次数最多的类别作为待分类样本的类别；或在回归问题中，取K个最近邻样本的平均值作为预测值。\n\nK-NN算法的优点包括：\n\n* **简单易实现**：K-NN算法不需要显式的训练过程，直接基于距离进行分类和回归。\n* **无参数模型**：K-NN算法不需要学习模型参数，适合于小规模的训练集。\n\nK-NN算法的缺点包括：\n\n* **计算量大**：对于每一个待分类样本，都需要计算与训练集中所有样本的距离，计算量较大。\n* **高维数据效果差**：在高维空间中，数据点之间的距离变得难以度量，K-NN算法效果不佳。\n* **对噪声敏感**：K-NN算法对噪声和异常值较为敏感，影响分类结果。\n\n尽管如此，K-NN算法在很多实际应用中依然表现出色，尤其是在分类问题中。例如，在手写数字识别、图像分类、文本分类等领域，K-NN算法具有较好的表现。\n\n## 非监督式学习[ #](#非监督式学习)\n\n非监督式学习是一种机器学习方法，其在训练过程中不需要使用标签的数据。这种学习方式允许算法对输入数据进行自我学习，并识别其中的模式和结构。它的目标通常是找出隐藏在数据中的模式或关系，如聚类或降维。因为没有预先设定的目标或输出变量，非监督式学习常常被用于探索性分析，以帮助研究者识别未知的模式或关系。\n\n它的典型特征是有输入数据，但是没有输出数据（无监督学习使用无标签的数据）。\n\n对于非监督学习，输入的数据都是混沌的数据，为了处理这些无标签的混沌数据，需要把它们进行聚类和降维，这样才便于提取有用的数据信息。\n\n常见的非监督算法，包含聚类算法（K-means算法），以及主成分分析（PCA）算法。\n\n### K-means算法[ #](#k-means算法)\n\nK-means算法是一种常见的非监督式学习算法，用于将数据集分成K个簇（clusters）。其工作原理是通过迭代优化的方法，将数据点分配到最接近的簇中心，最终使每个簇内的数据点之间的相似性最大化，而不同簇之间的相似性最小化。具体步骤包括：\n\n1. 随机选择K个初始簇中心。\n2. 将每个数据点分配到最接近的簇中心。\n3. 计算每个簇的新的中心点，即簇内所有数据点的均值。\n4. 重复步骤2和3，直到簇中心不再发生显著变化或达到预定的迭代次数。\n\nK-means算法简单高效，适用于大规模数据集，但需要预先指定K值，并且对初始簇中心的选择较为敏感。\n\n### 主成分分析（PCA）算法[ #](#主成分分析pca算法)\n\n主成分分析（PCA）是一种统计学上的处理方法，用于通过降维技术简化数据集。它的目的是找到最能代表原始数据分布的低维度空间。PCA的工作原理是通过创建一个新的坐标系统，将原始数据重新映射到这个新的坐标系统上。新坐标系统的基底由原始数据的主成分（即方差最大的方向）构成。在新的坐标系统中，大部分的信息都被压缩到了第一主成分上，第二大的信息被压缩到了第二主成分上，以此类推。这样，我们就可以通过保留前几个主成分，丢弃剩下的主成分，实现对数据的降维。PCA广泛应用于图像压缩、数据可视化、噪声过滤、特征抽取和数据预处理等领域。\n\n## 深度学习[ #](#深度学习)\n\n深度学习是一种机器学习的方法，它试图模拟人脑的工作方式，通过训练大量的数据，让计算机模型学习识别模式和进行高级决策。深度学习的核心是神经网络，特别是深度神经网络，它包含多层的隐藏层，每一层都在进行特定的数据处理。深度学习在语音识别、图像识别、自然语言处理等领域有广泛的应用，并带来了显著的效果。\n"}}