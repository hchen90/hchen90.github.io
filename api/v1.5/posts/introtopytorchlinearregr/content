{"code":0,"data":{"title":"使用 PyTorch 进行线性回归模型训练实战","content":"# 使用 PyTorch 进行线性回归模型训练实战\n\n# 使用 PyTorch 进行线性回归模型训练实战[ #](#使用-pytorch-进行线性回归模型训练实战)\n\n本文结合个人实验的Jupyter Notebook示例，从零开始使用**PyTorch**完成一个**线形回归模型的构建、训练与理解**。内容面向初学者，同时也帮助你建立对 PyTorch 自动求导与训练流程的正确认知。\n\n## 一、线性回归问题回顾[ #](#一线性回归问题回顾)\n\n线性回归是最基础的监督学习模型，其数学形式为：\n\ny^​=wx+b\n\n其中：\n\n* x：输入特征\n* w：权重（weight）\n* b：偏置（bias）\n* y^​：预测值\n\n模型训练的目标是通过数据学习合适的 w, b，使预测值 y^​ 尽可能接近真实值 y。\n\n## 二、PyTorch 基础：Tensor 与自动求导[ #](#二pytorch-基础tensor-与自动求导)\n\n### 什么是Tensor[ #](#什么是tensor)\n\n在Jupyter Notebook的环境里，先安装好torch，然后新建一个tensor示例，如下：\n\n![crttensorsim](/post/introtopytorchlinearregr/a571579617c9.png)\n\n这个是一个最简单的Tensor，它不带梯度，由一个矩阵数组构造而来。Tensor有几个重要属性，包括：`shape`，`dtype`，以及`device`。\n\n* `shape`: 它的含义就是矩阵的形状，几行几列。\n* `dtype`: 它表示元素的数据类型。\n* `device`: 它表示的是使用的设备类型，一般为`cpu`，或者`cuda`。\n\n> `device`为`cuda`的是启用Nvdia的CUDA机制，通过GPU进行计算，可以显著提高Tensor的计算效率。\n\n示例如下图：\n\n![tensorshape](/post/introtopytorchlinearregr/ffbe12c0bdd3.png)\n\n由此可见，Tensor是一种矩阵的数据封装器，那么再来探索一下这个矩阵封装器的其他特性。\n\n### Tensor的自动求导[ #](#tensor的自动求导)\n\n我们探索了 **Tensor 的创建与属性**，下面是一个做ML/LLM都会用到的参数`requires_grad`，这是创建自动求导Tensor的核心数据结构:\n\n![tensorautograd](/post/introtopytorchlinearregr/521dc31c8d4e.png)\n\n`requires_grad` 表示对Tensor进行计算时，可以跟踪它的计算过程，将来可以对它求导（梯度计算）。\n\n示例如下，首先使用Tensor进行系列计算：\n\n![tensorcompu](/post/introtopytorchlinearregr/6f168e362648.png)\n\n可以看见`a`，`b`，`c`，`d`，这几个Tensor变量，都带有grad\\_fn的属性，它表示该Tensor是通过什么运算得到的。\n\n### Tensor的反向传播[ #](#tensor的反向传播)\n\nTensor的反向传播机制提供了一种可以跟踪计算的能力，但是只能针对于标量。\n\n![tensotgradscal](/post/introtopytorchlinearregr/55f6edf4d802.png)\n\n从上图可以看出，`x`标量被进行一系列的计算，最终都可以通过`.grad`的属性来得到梯度斜率。\n\n> Tensor除了介绍的以外，还有中位数，最大数，求和等等，详情可以参考[官方资料](https://docs.pytorch.org/docs/stable/torch.html)。\n\n## 三、使用 `torch.nn.Linear` 构建线性层[ #](#三使用-torchnnlinear-构建线性层)\n\n这里需要使用到torch中的Neural Network模块的线形模型，核心是构建一个基本的线形层：\n\npythonCopy\n\n```python\nlinear_layer = torch.nn.Linear(in_features=1, out_features=1)\n\nprint(linear_layer.weight)\nprint(linear_layer.bias)\n```\n\n这一步等价于定义公式：\n\ny=wx+b\n\n其中：\n\n* `weight` 对应 w\n* `bias` 对应 b\n\n## 四、定义线性回归模型[ #](#四定义线性回归模型)\n\n定义线形回归模型分为多个步骤。\n\n### 第一步：准备好数据[ #](#第一步准备好数据)\n\n这里的数据是用随机数生成公式中的`X`，然后通过基本计算加上一点随机数模拟噪音，以此模拟现实环境。\n\n![regrdata](/post/introtopytorchlinearregr/8d73bda6f09c.png)\n\n其中，`true_W`和`true_b`是理论值，`y_true`是模拟结果值，我们通过数据模型训练找到接近理论值的`W`，'b'。\n\n> 对应现实情况下，`X`可能是仪器的输入值（自变量），`y_true`是历史记录中的关于`X`对应的结果值（应变量），这段数据就是机器训练用的数据，现实情况中该数据量会很大。\n\n### 第二步：定义回归模型[ #](#第二步定义回归模型)\n\n到这里了，实现的方法不是唯一的，虽然一般推荐使用继承`torch.nn.Module`的方式来定义模型，但是便于说明其内部原理，这里使用函数式，参数计算也是自己手动计算。\n\n![regrmodelfd](/post/introtopytorchlinearregr/a76db5f8343b.png)\n\n就是这么一个loop，我们就完成了模型训练。\n\n## 五、Forward Pass（前向传播）[ #](#五forward-pass前向传播)\n\n### 什么是前向传播[ #](#什么是前向传播)\n\n前向传播就是通过模型输入数据，进而得到预测结果。\n\nCopy\n\n```\n输入数据 → 模型 → 得到预测结果\n```\n\n> 这里的模型是不准确的，得到的结果可能是错误的，需要进一步参数调整，才能提高准确度。\n\n### 对应代码解释[ #](#对应代码解释)\n\n下面是代码说明，具体代码：\n\npythonCopy\n\n```python\n# forward pass and loss\ny_hat = X @ W + b\n```\n\n这里PyTorch自动构建计算图，并且记录计算关系（可以参考上面Tensor内容介绍）。\n\n## 六、定义损失函数（Loss Function）[ #](#六定义损失函数loss-function)\n\n线性回归中最常用的是 **均方误差（MSE）**：\n\npythonCopy\n\n```python\nloss = torch.mean((y_hat - y_true) ** 2)\n```\n\n数学形式为：\n\nMSE=N1​∑(y−y^​)2\n\n损失值用于衡量模型预测的好坏，可见损失量越小越好。\n\n## 七、Backward Pass（反向传播）[ #](#七backward-pass反向传播)\n\n### 什么是反向传播[ #](#什么是反向传播)\n\n在前向传播时，构建了计算图，并且记录了计算关系。反向传播就是沿着计算图的相反方向传播，计算所有`requires_grad=True`的参数梯度。\n\n### 对应代码解释[ #](#对应代码解释-1)\n\n反向传播只有一行代码；\n\npythonCopy\n\n```python\n# backward pass\nloss.backward()\n```\n\n通过这个反向函数调用，对于之前的两个参数：`W`，`b`。\n\n我们可以将这两个参数的梯度斜率算出来。\n\n## 八、参数更新[ #](#八参数更新)\n\n参数更新的核心代码片段：\n\npythonCopy\n\n```python\n# update parameters\nwith torch.no_grad():\n    W -= learning_rate * W.grad\n    b -= learning_rate * b.grad\n\n# zero gradients\nW.grad.zero_()\nb.grad.zero_()\n```\n\n这里通过微调参数来减小上面的损失率，从而达到参数（`W`，`b`）接近理论值。\n\n这里有两点注意：\n\n* 为了使微调`W`，`b`时，不会把计算记录到计算图上，这里使用了`torch.no_grad`函数，在它作用下，计算Tensor不会记录到计算图上。\n* 在完成参数更新后，我们需要重置`W`和`b`的梯度，避免累加到下次循环，从而产生混乱。\n\n## 九、更高级的写法[ #](#九更高级的写法)\n\n对于实际场景下，一般模型训练不会像上面这样写，对于只有两个参数可以手动写写，但是一般LLM模型有7B，256B等等，这么多参数这样去写一点都不现实，对于手动计算那块代码，在torch中其实有现成的实现，除此之外它还有更多的内容，下面是展示一个更加通用的写法（使用继承`torch.nn.Module`的方式来定义模型，并且使用torch内部的实现机制）。\n\n![regrmodel](/post/introtopytorchlinearregr/c28cf58dab1a.png)\n\n因为torch中对于常规的模型，优化器，损失函数都有定义，按照这种约定俗成的方式，对于参数更新那块全部被隐藏到了优化器中，处理非常优雅。\n\n## 十、总结[ #](#十总结)\n\n通过本文的实战示例，我们从 **最基础的数学模型** 出发，完整走了一遍使用 PyTorch 训练线性回归模型的全过程，并逐步揭开了 PyTorch 自动求导与训练机制的“黑盒”。\n\n核心收获可以归纳为以下几点：\n\n1. **线性回归的本质并不复杂** 无论是数学公式\n\n   y=wx+b\n\n   还是 PyTorch 中的 `nn.Linear`，本质都是对这一公式的工程化封装。理解这一点，有助于你在学习更复杂模型时始终保持“模型直觉”。\n\n2. **Tensor 是 PyTorch 的核心数据结构** Tensor 不只是一个“多维数组”，当 `requires_grad=True` 时，它会成为计算图中的节点，自动参与梯度计算，这是 PyTorch 能够高效训练模型的基础。\n\n3. **Forward / Backward 构成了训练闭环**\n\n   * Forward Pass：根据当前参数计算预测值与损失\n   * Backward Pass：根据损失反向计算梯度\n   * Parameter Update：利用梯度更新参数\n\n   这一流程不仅适用于线性回归，也适用于几乎所有深度学习模型。\n\n4. **理解“手写训练循环”非常重要** 虽然在实际工程中我们通常会使用：\n\n   * `nn.Module`\n   * `torch.optim`\n   * `nn.MSELoss`\n\n   但通过手动实现参数更新与梯度清零，你可以真正理解：\n\n   * 梯度从哪里来\n   * 为什么要 `zero_grad()`\n   * 为什么参数更新要放在 `torch.no_grad()` 中\n\n5. **高级封装是建立在底层原理之上的** PyTorch 提供的高级 API 并不是“魔法”，而是对这些基础流程的高度抽象。当你理解了底层逻辑，再回过头看标准写法，会发现它既优雅又可靠。\n\n总的来说，**线性回归是学习 PyTorch 和深度学习的最佳起点**。只要你真正理解了本文中的每一步，那么无论是多层神经网络、CNN、RNN，还是今天流行的大模型（LLM），它们在训练流程上都是一脉相承的。\n\n后续你可以尝试的方向包括：\n\n* 使用真实数据集（如房价、时间序列）\n* 引入 `Dataset` / `DataLoader`\n* 将模型迁移到 GPU（CUDA）\n* 扩展到多维特征或多层网络\n\n理解基础，才能走得更远。\n"}}