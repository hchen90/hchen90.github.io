{"code":0,"data":{"title":"人工智能笔记 - 数据分析和决策","content":"# 人工智能笔记 - 数据分析和决策\n\n# 人工智能 - 数据分析和决策[ #](#人工智能---数据分析和决策)\n\n这部分主要分为四个层次：\n\n* 数据收集\n* 数据清理\n* 数据分析\n* 决策\n\n## 数据收集[ #](#数据收集)\n\n在人工智能领域数据收集是至关重要的一个环节，它是用于训练模式的输入来源。\n\n作为基础的概念复习，数据收集无非是把目标进行采样，同时我们需要使用科学的方法使样本接近于目标。\n\n作为采样的方法，通常可以分为三种：\n\n1. 简单随机采样\n2. 系统采样\n3. 分层采样\n\n简单随机采样最好理解，使用随机数发生器对目标空间随机采取样本；对于系统采样，通常强调一种顺序性质；但是往往我们的真实世界并不都是顺序可以表示的，所以这时可以使用分层采样来进行，它先对目标进行分类，在类别中可以结合其他采样方式，例如：系统采样或简单随机采样，这种采样玩玩更具有目标的参考意义。\n\n采样方法虽分三种，但是采样途径却有各种方式，常见的有以下：\n\n* 调查\n* 问卷\n* 采访，或焦点小组\n* 交易跟踪\n* 在线跟踪\n* 社交媒体监控\n\n收集数据要遵循正式行，合法性，以及道德性。\n\n## 数据清理[ #](#数据清理)\n\n数据清理是为了让样本数据更加准确地反映目标，对于清理首先需要定义变量，这样进而便于操作。\n\n### 数据类型[ #](#数据类型)\n\n数值型数据\n\n* 离散数据\n* 连续数据\n\n分类型数据\n\n* 序列数据\n* 名义数据\n\n离散数据是个数形式，它是确定的整数，而连续数据是一种可以无限精确的数值，它是浮点数；对于序列数据是数据集之间存在关联性，而名义数据则没有。\n\n### 数据收集准则[ #](#数据收集准则)\n\n* 避免偏见\n\n  数据偏见要遵从数据来源的确定性，它要能真实反映实际情况，除此之外，数据的采样方法和测量手段也同等重要。\n\n* 数据表示\n\n  数据表示一般使用表格形式，例如横轴表示记录变量值，纵轴表示采样单元属性。\n\n* 数据纠正\n\n  数据纠正需要去除重复或者拼写错误的数据记录，尽可能地使数据有效性最大化。\n\n* 数据强化\n\n  数据强化是增加更多的采样单元属性，这样可以收集更多样本属性。\n\n* 偏离值\n\n  偏离值是一种采样中出现的错误数据，对于这种偏离值需要进行删除操作。\n\n数据清理最重要的一个环节就是去除偏离值。\n\n## 数据分析[ #](#数据分析)\n\n数据分析之前，需要明白一些基本的数学概念，这样便于理解数据的分布结构，更好地进行数据分析。\n\n### 基本数据类型[ #](#基本数据类型)\n\n* 众数\n\n  出现频数最多的数值。\n\n* 均值\n\n  样本空间的数据平均值。\n\n  μ=n1​i=1∑n​xi​\n\n* 中位数\n\n  样本空间中处于中间的数值，对于奇数个数的样本，它就是最中间的数值，对于偶数个数的样本，它是中间两个样本的平均值。\n\n* 极差\n\n  样本中最大数值和最小数据的差值。\n\n  Range=max(x)−min(x)\n\n* 方差\n\n  方差由标准的方差公式而来，方差值越大说明样本数据越不能收敛。\n\n  σ2=n1​i=1∑n​(xi​−μ)2\n\n* 标准差\n\n  标准差是方差开根号后的正数，性质类似于方差。\n\n  σ=+n1​i=1∑n​(xi​−μ)2​\n\n### 概率论原理[ #](#概率论原理)\n\n概率论的基本性质：\n\n* 假设S为样本的全集。\n\nS=S\n\n* 假设A和B为样本集的子集。\n\nA,B⊂S\n\n* 假设A和B没有交集。\n\nA∩B=∅\n\n可以得到三个公理：\n\n1. 对于任何事件A，其概率符合非负性。\n\n   P(A)≥0\n\n2. 对于必然发生的事件，其概率为1。\n\n   P(S)=1\n\n3. 对于任意多个互不相容的事件，它们的并的概率等于它们各自概率的和。\n\n   P(A∪B)=P(A)+P(B)\n\n概率分布通常分为两种：\n\n* 离散分布，对应于概率质量函数（PMF）\n* 连续分布，对应于概率密度函数（PDF）\n\n### 常见的分布函数[ #](#常见的分布函数)\n\n* 简单离散分布\n\n  不存在自由变量，概率发生是等可能事件，例如：抛出的硬币出现正面或者反面的概率。\n\n* 二项式分布\n\n  二项式分布是一种离散分布，通常具有以下四点性质：\n\n  * 重复多次性质（用N表示）\n  * 结果独立，结果之间互不影响\n  * 只有两个结果，结果没有交集\n  * 概率值不会\n\n  P(X=k)=(kn​)pk(1−p)n−k\n\n  对于二项式分布，它属于概率质量函数（PMF），它的自由变量是p。\n\n* 高斯分布\n\n  高斯分布又称正态分布，它是用于连续分布的常见模型。\n\n  f(x)=σ2π​1​exp(−2σ2(x−μ)2​)\n\n  高斯分布函数属于概率密度函数（PDF），它的自由变量为均值μ和标准差σ。\n\n* 泊松分布\n\n  泊松分布用于统计单位时间或空间段事件发生的概率。泊松分布的特点是，事件的发生是相互独立的，即前后事件之间没有影响关系。在实际应用中，泊松分布被广泛用于服务系统的模型设计、排队论、电信等领域。\n\n  P(X=k)=k!λke−λ​\n\n  泊松分布属于概率质量函数（PMF），它的自由变量是λ。\n\n## 决策[ #](#决策)\n\n上面只是介绍了数据分析的各种数学分析方法和相应的数学模型，对于人工智能来说，至关重要的环节是进行决策判断，因此我们使用的样本要保证能够准确代表目标，这样才能做出正确的决策。\n\n这里我们使用数学原理，来进行模拟和验证，以确定样本的数学模型。\n\n### 估算原理[ #](#估算原理)\n\n估算原理主要以介绍最大似然估计（MLE），最大似然估计方法分为五个步骤：\n\n1. 设定模型和参数。\n\n   对于数学模型，可以通过数据观察或经验，来选定一个数学模型，例如估算目标体只有两个结果的，我们可以假设为二项式分布，那么它的自由参数就是p。\n\n2. 写出似然函数。\n\n   对于二项式分布的概率质量函数（PMF）为：\n\n   P(X=k)=(kn​)pk(1−p)n−k\n\n   其似然函数为：\n\n   L(p)=i=1∏m​P(X=i)=i=1∏m​(kn​)pk(1−p)n−k\n\n3. 取对数似然函数。\n\n   再计算对数似然函数：\n\n   ℓ(p)=logL(p)=log(i=1∏m​(kn​)pk(1−p)n−k)=i=1∑m​(log(kn​)+klogp+(n−k)log(1−p))\n\n   化解求得：\n\n   ℓ(p)=i=1∑m​(klogp+(n−k)log(1−p))\n\n4. 求解参数的极值。\n\n   dpdℓ(p)​=i=1∑m​(pxi​​−1−pn−xi​​)=0\n\n   化解求得：\n\n   p1​i=1∑m​xi​=1−p1​i=1∑m​(n−xi​)\n\n   假设：\n\n   i=1∑m​xi​=S\n\n   可解析得到：\n\n   p=mnS​\n\n5. 使用似然估计方法计算自由变量参数。\n\n   似然估计值：\n\n   p^​=mn∑i=1m​xi​​\n\n## 统计假设验证[ #](#统计假设验证)\n\n通过上节的方法，可以通过似然估计得到自由变量参数的值，但是对于当时选择的数学模型是否正确，是否与实际的目标空间相匹配，并没有进行相关讨论。\n\n其实很多时候，我们选择的数学模型可能是错误，我们可能需要多个数学模型依次尝试才能确定相符合的数学模型，这章节就是使用统计验证假设方法，从而让我们的模型准确匹配目标，这样人工智能才可以做出正确的决策。\n\n### 统计假设检验的步骤[ #](#统计假设检验的步骤)\n\n统计假设检验是一个决定接受或拒绝统计假设的过程。以下是进行统计假设检验的一般步骤：\n\n1. 建立假设：首先，我们需要设置两个假设。零假设（H0）通常表示原始的，被测试的理论。而备择假设（H1）则是我们想要证明的新理论。\n\n2. 决定显著性水平：显著性水平（通常记为α）是你愿意接受的犯第一类错误（即拒真错误，错误地拒绝零假设）的概率。常见的显著性水平有0.05和0.01。\n\n3. 选择适当的统计检验：根据你的数据类型和目标，选择一个适当的统计检验。例如，如果你要比较两组数据的平均值，你可能会选择t检验。\n\n4. 确定临界值或计算 p 值：\n\n   * **临界值法**：将计算得到的检验统计量与临界值进行比较。如果检验统计量超出临界值，则拒绝原假设。\n   * **p 值法**：如果 p 值小于显著性水平 α，则拒绝原假设。\n\n5. 结论：如果我们拒绝了零假设，那么我们就接受了备择假设，认为我们的新理论是对的。如果我们没有拒绝零假设，那么我们就说没有足够的证据来支持我们的新理论，但这并不意味着我们的旧理论就一定是正确的。\n"}}